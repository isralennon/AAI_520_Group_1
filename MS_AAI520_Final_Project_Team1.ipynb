{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7846f45-e90c-44c2-8ad1-e1f8db4e3325",
   "metadata": {},
   "source": [
    "# UNIVERSITY OF SAN DIEGO - MS AAI\n",
    "## Natural Language Processing and Generative AI\n",
    "### Final Project - Team 1: Multi-Agent Financial Analysis System.\n",
    "#### By Manikandan Perumal & Israel Romero Olvera\n",
    "#### _________________________________________________\n",
    "#### The purpose of this final project is to build a real-world financial analysis system powered by agentic AI, with the abilities of reasoning, planning, and acting based on the user's prompt. It will coordinate multiple specialized LLM agents to handle complex financial tasks end-to-end.\n",
    "#### Our Agentic AI system was developed in a folder structure that can be found in our GitHub site: https://github.com/isralennon/AAI_520_Group_1/tree/main\n",
    "#### For delivery purposes we've condensed all the code into this document, structured the following way:\n",
    "#### 1. Tools - this section contains the code in file /modules/tools.py which will perform basic RAG connections.\n",
    "#### 2. Parser - this section contains the code in file /modules/parser.py, which provides basic functionality to parse data in JSON format.\n",
    "#### 3. Memory - this section contains the code in file /modules/memory.py that handles the storage of ongoing knowledge, to provide a robust and efficient functionality.\n",
    "#### 4. Agents - this section contains the code in file /modules/subagents.py, designed to host the definitions of the main Agent class as well as our specialized subagents - the team of agents available to the main orchestrator. We developed the following team of agents:\n",
    "#### - Orchestrator - the \"Manager\" of the Agents\n",
    "#### - News Researcher - the specialist of finding financial news, using FinnHub.\n",
    "#### - Market Researcher - the specialist of finding financial hard data like market trends, stock prices, etc.\n",
    "#### - Writer - the specialist of taking all the information and preparing a polished answer for the user\n",
    "#### 5. Main Orchestrator Agent - this section contains the code in file /modules/agent.py and has the definition for the orchestrator agent, which develops the strategy and coordinates all subagents.\n",
    "#### 6. Demo - this section contains the code in our main notebook, /main.ipynb - our implementation file where we execute all the above with demonstration purposes.\n",
    "#### _________________________________________________\n",
    "### 1. TOOLS.\n",
    "#### One of the four agent functions we'll implement is the usage of tools, which will be defined in this first section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5847078-4728-443e-9d7f-3a350066678f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isral\\.conda\\envs\\Transformers_3_10\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "#import modules.tools as tools\n",
    "import yfinance as yf\n",
    "import requests\n",
    "import finnhub\n",
    "from typing import Callable\n",
    "from datetime import datetime, timedelta\n",
    "from google import genai\n",
    "import openai\n",
    "\n",
    "# For privacy reasons, we'll store our token keys on a .env file, which we'll load here:\n",
    "dotenv.load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "# First, we'll define a generic Tool class, which will serve as a structure for all of our tools\n",
    "class Tool:\n",
    "    def __init__(self, name, function, description, api=None): # This is the initialization method of the class\n",
    "        self.name = name # Placeholder for the name of the tool\n",
    "        self.function = function # Placeholder for the code of the tool's function\n",
    "        self.description = description # Placeholder for the description of the tool - very important since the agents will use this description to know what the tool does\n",
    "        self.api = api  # Placeholder for API details when needed\n",
    "        \n",
    "    def to_dict(self): # The structure of each class will always be a standard dictionary object that can be easily interpreted by the Agents\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"description\": self.description,\n",
    "            \"api\": self.api\n",
    "        }\n",
    "    \n",
    "    def invoke(self, **kwargs): # This is the placeholder of the function for the tool, which will receive a variable number of parameters\n",
    "        print(f\"Invoking {self.name} with arguments {kwargs}\")\n",
    "        return self.function(**kwargs) # Returning the results of the function\n",
    "\n",
    "# Next, we'll declare each individual tool as a class, inheriting from the generic class Tool above\n",
    "class YahooFinance(Tool): # The first tool is YahooFinance, which will pull stock quotes for a given financial symbol, like AAPL for Apple\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"Yahoo Finance Stock Quote\", # Name of the tool\n",
    "            function=self.get_stock_quote_yahoo, # Pointing to the YahooFinance function below as this class's own function\n",
    "            description=\"Get the latest stock quote for a given symbol from Yahoo Finance.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"\"symbol\": \"AAPL\"}\"\"\", # Parameter sample for the agent to use when it uses this class\n",
    "        )\n",
    "    def get_stock_quote_yahoo(self, symbol: str, step: str='') -> dict: # This is the function that pulls the stock using YahooFinance API\n",
    "        # Here we'll perform the call to YahooFinance to get the data from the specified symbol.\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        # Then, we'll use the 'fast_info' method, which pulls basic financial information, including the price.\n",
    "        try:\n",
    "            info = ticker.fast_info # Pulling the information and parsing it to return it\n",
    "            return {\n",
    "                \"symbol\": symbol,\n",
    "                \"last_price\": info[\"lastPrice\"],\n",
    "                \"day_high\": info[\"dayHigh\"],\n",
    "                \"day_low\": info[\"dayLow\"],\n",
    "                \"previous_close\": info[\"previousClose\"]\n",
    "            }\n",
    "        except Exception as e: # Should there be any errors, we will print the error message instead and return an empty dictionary\n",
    "            print(f\"Yahoo Finance API error: {e}\")\n",
    "            return {}\n",
    "#Now, we'll continue with the class that calls Financial Modeling Prep API\n",
    "class FMP(Tool):\n",
    "    def __init__(self,name:str,function:Callable=None,description:str=None,api:str=None,endPoint:str=None):\n",
    "        super().__init__(name=name,function=self.execute if function==None else function,description=description,api=api)\n",
    "        self.endpoint = endPoint if endPoint!=None else  os.getenv(\"FMP_Endpoint\") # It reads the endpoint from our .env file\n",
    "        self.apikey = os.getenv(\"FMP_API_KEY\") # It also reads the API key from our .env file\n",
    "    def execute(self, symbol: str) -> dict: # This is the function that pulls the stock data using FMP API\n",
    "        params = { #These are the parameters for the API call in a dictionary format\n",
    "            \"symbol\": symbol,\n",
    "            \"apikey\": self.apikey,\n",
    "            \"exchange\": \"NASDAQ\"\n",
    "        }\n",
    "        try: #Then we'll try to make the call to the API and return its formatted response as a JSON text\n",
    "            # print(f'Calling FMP API at endpoint: {self.endpoint} with params: {params}')\n",
    "            response=requests.get(self.endpoint, params=params)\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e: # Should there be any errors, we'll print the error message and return an empty dictionary\n",
    "            print(f'FMP API error: {e}')\n",
    "            return {}\n",
    "        \n",
    "class StockQuote(FMP):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"Stack Quote\", # Name of the tool\n",
    "            description=\"Get the latest stock quote for a given symbol from Stack Quote.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"symbol\": \"AAPL\"}\"\"\", # Parameter sample for the agent to use when it uses this class\n",
    "            endPoint='https://financialmodelingprep.com/stable/quote' # It reads the endpoint from our .env file\n",
    "        )\n",
    "\n",
    "        \n",
    "class StockPriceChange(FMP):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"Stock Price Change\", # Name of the tool\n",
    "            description=\"Get the stock price change for a given symbol over the past.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"symbol\": \"AAPL\", \"days\": 7}\"\"\", # Parameter sample for the agent to use when it uses this class\n",
    "            endPoint='https://financialmodelingprep.com/stable/stock-price-change' # It reads the endpoint from our .env file\n",
    "        )\n",
    "        \n",
    "class IncomeStatement(FMP):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"Income Statement\", # Name of the tool\n",
    "            description=\"Get the income statement for a given symbol from Financial Modeling Prep (FMP).\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"symbol\": \"AAPL\"}\"\"\", # Parameter sample for the agent to use when it uses this class\n",
    "            endPoint='https://financialmodelingprep.com/stable/income-statement' # It reads the endpoint from our .env file\n",
    "        )\n",
    "  \n",
    "class FinancialScore(FMP):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"Financial Score\", # Name of the tool\n",
    "            description=\"Get the financial score for a given symbol from Financial Modeling Prep (FMP).\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"symbol\": \"AAPL\"}\"\"\" ,# Parameter sample for the agent to use when it uses this class\n",
    "            endPoint='https://financialmodelingprep.com/stable/financial-scores' # It reads the endpoint from our .env file\n",
    "        )\n",
    "   \n",
    "        \n",
    "#We'll be using FinnHub as our News provider next\n",
    "class FinancialNews(Tool): \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"FinnHub News\", # Name of the tool\n",
    "            function=self.get_stock_quote_finnhub, # Pointing to the FinnHub function below as this class's own function\n",
    "            description=\"Get the latest financial news for a given symbol from FinnHub.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"\"symbol\": \"AAPL\"}\"\"\" # Parameter sample for the agent to use when it uses this class\n",
    "        )\n",
    "    def get_stock_quote_finnhub(self, symbol: str, step: str='') -> dict: # This is the function that pulls the news data using FinnHub\n",
    "        FinnHubAPIKey = os.getenv(\"FINNHUB_API_KEY\") # Gets the API key from our .env file\n",
    "        # Next, we setup the client to perform calls:\n",
    "        finn_client = finnhub.Client(api_key=FinnHubAPIKey)\n",
    "\n",
    "        # Setting a time frame for the news, ending today and starting a week ago\n",
    "        end_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "        start_date = (datetime.today() - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Now, we call the API, returning the news in the already pre-formatted dictionary structure.\n",
    "        try:\n",
    "            news= finn_client.company_news(symbol, _from=start_date, to=end_date)\n",
    "            if len(news)==0:\n",
    "                return {\"message\": f\"No news found for symbol {symbol} from {start_date} to {end_date}.\"}\n",
    "            top_news = sorted(news, key=lambda x: x['datetime'], reverse=True)[:5]\n",
    "            top_news_formatted = []\n",
    "            for item in top_news:\n",
    "                top_news_formatted.append({\n",
    "                    \"headline\": item.get(\"headline\"),\n",
    "                    \"summary\": item.get(\"summary\"),\n",
    "                    \"datetime\": datetime.fromtimestamp(item.get(\"datetime\")).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                })\n",
    "        \n",
    "            return {\n",
    "                \"symbol\": symbol,\n",
    "                \"news\": top_news_formatted  \n",
    "            }\n",
    "    \n",
    "        except Exception as e: # Should there be any errors, we'll print the error message and return an empty dictionary\n",
    "            print(f'Finnhub.io API error: {e}')\n",
    "            return {}\n",
    "\n",
    "class RecommendationTrends(Tool):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"FinnHub Recommendation Trends\", # Name of the tool\n",
    "            function=self.get_recommendation_trends, # Pointing to the FinnHub function below as this class's own function\n",
    "            description=\"Get the recommendation trends for a given symbol from FinnHub.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"\"symbol\": \"AAPL\"}\"\"\" # Parameter sample for the agent to use when this class\n",
    "        )\n",
    "    def get_recommendation_trends(self, symbol: str) -> dict:\n",
    "        FinnHubAPIKey = os.getenv(\"FINNHUB_API_KEY\") # Gets the API key from our .env file\n",
    "        finn_client = finnhub.Client(api_key=FinnHubAPIKey)\n",
    "        try:\n",
    "            return finn_client.recommendation_trends(symbol)\n",
    "        except Exception as e: # Should there be any errors, we'll print the error message and return an empty dictionary\n",
    "            print(f'Finnhub.io API error: {e}')\n",
    "            return {}\n",
    "        \n",
    "class EarningSurprise(Tool):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"FinnHub Earning Surprise\", # Name of the tool\n",
    "            function=self.get_earning_surprise, # Pointing to the FinnHub function below as this class's own function\n",
    "            description=\"Get the earning surprise for a given symbol from FinnHub.\", # Definition of the tool for our agents\n",
    "            api=\"\"\"{ \"\"symbol\": \"AAPL\"}\"\"\" # Parameter sample for the agent to use when this class\n",
    "        )\n",
    "    def get_earning_surprise(self, symbol: str) -> dict:\n",
    "        FinnHubAPIKey = os.getenv(\"FINNHUB_API_KEY\") # Gets the API key from our .env file\n",
    "        finn_client = finnhub.Client(api_key=FinnHubAPIKey)\n",
    "        try:\n",
    "            return finn_client.company_earnings(symbol,limit=5)\n",
    "        except Exception as e: # Should there be any errors, we'll print the error message and return an empty dictionary\n",
    "            print(f'Finnhub.io API error: {e}')\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd7b2a-b7e4-45df-8551-153c0d0da84b",
   "metadata": {},
   "source": [
    "### 2. PARSER\n",
    "#### One of the workflow patterns our agents will do is routing, meaning our main agent will coordinate with subagents. To accomplish this communication, we need a \"common language\", which in this case will be JSON. This section defines the functions to implement the JSON parsing functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa5fdb39-7ca7-4dca-8fb9-6173fa06dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll define first a Parser abstract class\n",
    "class Parser:\n",
    "    def parse(self, response): # This is the placeholder of the default method for this class\n",
    "        # Here's the returned value, which will be a dictionary with an Action value, and a list of dynamic parameters.\n",
    "        return {\"action\": \"FinalAnswer\", \"parameters\": {}}\n",
    "# Next, we'll define an XML parser, which inherits from our abstract class Parser.    \n",
    "class XmlParser(Parser):\n",
    "    def parse(self, response):\n",
    "        # A parser that extracts XML tags from the response.\n",
    "        # For example, it looks for <InvokeTool>{\"symbol\": \"AAPL\", \"step\": \"financials\"}</InvokeTool>\n",
    "        # or <FinalAnswer>answer</FinalAnswer>.\n",
    "        # Returns : a dict with action and parameters. Example:\n",
    "        # {\n",
    "        #    \"action\": \"InvokeTool\",\n",
    "        #    \"parameters\": {\n",
    "        #        \"symbol\": \"AAPL\",\n",
    "        #        \"step\": \"financials\"\n",
    "        #    }\n",
    "        #}\n",
    "        import re\n",
    "        pattern = r'<(\\w+)>(.*?)</\\1>' # Defining the regular expression for XML structure\n",
    "        matches = re.findall(pattern, response) # Identifying all matches of XML\n",
    "        if matches: # When there are XML matches, we'll separate them and parse their contents\n",
    "            action, content = matches[0]\n",
    "            content = content.strip()\n",
    "            contentJson = {}\n",
    "            try:\n",
    "                import json\n",
    "                contentJson = json.loads(content) # Once parsed, we'll reformat them to JSON\n",
    "            except:\n",
    "                contentJson = {\"content\": content} # If the content is not valid, we'll return the error message with the invalid content text\n",
    "                return {\"action\": action, \"parameters\": contentJson, \"error\": \"Content is not valid JSON\"}\n",
    "            return {\"action\": action, \"parameters\": contentJson} # If it was valid, we return the parsed content in JSON format\n",
    "        return {\"action\": \"FinalAnswer\", \"parameters\": {}} #If there wasn't any XML to begin with, we just return an empty list of parameters\n",
    "    # Next, we have a specialized parsing for our agent's functionality that will interpret the actions in XML tags and encode them as a list of dictionaries\n",
    "    def parse_all(self, response):\n",
    "        import re\n",
    "        pattern = r'<(\\w+)>(.*?)</\\1>' # Defining the regular expression for XML structure\n",
    "        matches = re.findall(pattern, response) # Identifying all matches of XML\n",
    "        results = [] # Preparing an empty array for the results\n",
    "        for action, content in matches: # For each detected action (if any),\n",
    "            content = content.strip()   # we'll parse its contents\n",
    "            contentJson = {}\n",
    "            try: # Then, we'll try to convert it to JSON format\n",
    "                import json\n",
    "                contentJson = json.loads(content)\n",
    "            except: # Should any errors occur, we'll return the error message as part of the response\n",
    "                contentJson = {\"content\": content}\n",
    "                results.append({\"action\": action, \"parameters\": contentJson, \"error\": \"Content is not valid JSON\"})\n",
    "                continue\n",
    "            results.append({\"action\": action, \"parameters\": contentJson}) # If everything's fine, we'll return the parsed JSON content\n",
    "        if not results:\n",
    "            results.append({\"action\": \"FinalAnswer\", \"parameters\": {}}) # If there were no actions, we'll return an empty dictionary\n",
    "        return results  \n",
    "    \n",
    "    def parseTags(self, response):\n",
    "        '''Agent response parser to extract all TAGS.\n",
    "            Returns a dictionary with tag names as keys and tag values as values.\n",
    "        '''\n",
    "        import re\n",
    "        pattern = r'<(\\w+)>(.*?)</\\1>'\n",
    "        matches = re.findall(pattern, response)\n",
    "        result = {}\n",
    "        for tag, value in matches:\n",
    "                result[tag.lower()] = value.strip() \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0e958-92da-4165-acd4-d541e967c259",
   "metadata": {},
   "source": [
    "### 3. MEMORY.\n",
    "#### Another feature of our agent is learning, which means the agent must remember information as it gets prompted to refine their answers and keep getting more knowledgeable as it gets used. The functions that perform such learning are defined in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5315f5-f806-4c34-87f7-f4187f6baa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "# We're creating a class called MemorySystem with all the learning functionality\n",
    "class MemorySystem:\n",
    "    # This class stores insights and lessons from previous analyses to improve future runs.\n",
    "    def __init__(self, memory_file='agent_memory.pkl'): # It will store the learned data into the specified file, or the default file name.\n",
    "        self.memory_file = memory_file\n",
    "        self.stock_insights = {}\n",
    "        self.news_insights = {}\n",
    "        self.load_memory()\n",
    "    \n",
    "    def load_memory(self): # Should there be a previous file in existence, it can load it using this function\n",
    "        try:\n",
    "            if os.path.exists(self.memory_file): # It will look for the file name specified in the instance of this class\n",
    "                with open(self.memory_file, 'rb') as f: # If it exists, it will attempt to open it\n",
    "                    memory_data = pickle.load(f) # Then, it will load the data into memory\n",
    "                    self.stock_insights = memory_data.get('stock_insights', {}) # separating stock insights,\n",
    "                    self.news_insights = memory_data.get('news_insights', {}) # market news insights,\n",
    "            else: # Should there be no prior file, it will start fresh\n",
    "                print(\"No memory file found. Starting with empty memory.\")\n",
    "        except Exception as e: # Should there be an error while loading the file, it will start fresh as well\n",
    "            print(f\"Error loading memory: {e}\")\n",
    "            print(\"Starting with empty memory.\")\n",
    "    \n",
    "    def save_memory(self): # This method will save the memory in the file in a structured manner\n",
    "        try:\n",
    "            memory_data = {\n",
    "                'stock_insights': self.stock_insights, # It will save all stock insights currently provided,\n",
    "                'news_insights': self.news_insights # followed by news insights\n",
    "            }\n",
    "            with open(self.memory_file, 'wb') as f: # It will first open the file name specified in the instance of this class\n",
    "                pickle.dump(memory_data, f) # and then write in it the contents of the memory_data dictionary\n",
    "            print(\"Memory saved successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving memory: {e}\") # Should there be any errors saving, it will print out the error\n",
    "    \n",
    "    def add_stock_insight(self, symbol, insight, timestamp=None): # With this method, we'll add knowledge classified as stock insights\n",
    "        if timestamp is None:\n",
    "            timestamp = datetime.now().isoformat() # If no timestamp is specified, we'll initialize the current time stamp\n",
    "        \n",
    "        if symbol not in self.stock_insights: # If the current symbol (financial company) is not in previous insights, we'll add it\n",
    "            self.stock_insights[symbol] = []\n",
    "        \n",
    "        self.stock_insights[symbol].append({ # Finally, we encode the insight with its timestamp in the stock_insights dictionary of this class\n",
    "            'insight': insight,\n",
    "            'timestamp': timestamp\n",
    "        })\n",
    "        self.save_memory() # And we save the memory right away\n",
    "    \n",
    "    def add_market_news(self,symbol, news_item, timestamp=None): # This method adds market news insights for a given symbol\n",
    "        if timestamp is None:\n",
    "            timestamp = datetime.now().isoformat() # If no timestamp is specified, we'll initialize the current time stamp\n",
    "\n",
    "        if symbol not in self.news_insights: # If the current symbol (financial company) is not in previous insights, we'll add it\n",
    "            self.news_insights[symbol] = []\n",
    "\n",
    "        self.news_insights[symbol].append({ # Finally, we encode the news item with its timestamp in the news_insights dictionary of this class\n",
    "            'news_item': news_item,\n",
    "            'timestamp': timestamp\n",
    "        })\n",
    "        self.save_memory() # And we save the memory right away\n",
    "\n",
    "    def get_stock_insights(self, symbol): # This method retrieves all stock insights for a given symbol\n",
    "        results=self.stock_insights.get(symbol, [])\n",
    "        if not results:\n",
    "            print(f\"No insights found for symbol {symbol}.\")\n",
    "            return []\n",
    "        if results:\n",
    "            filtered_results = []\n",
    "            for result in results:\n",
    "                # if the timestamp is older than 7 days, we can choose to ignore it\n",
    "                timestamp = datetime.fromisoformat(result['timestamp'])\n",
    "                if (datetime.now() - timestamp).days > 7:\n",
    "                    continue\n",
    "                filtered_results.append(result)\n",
    "        return filtered_results\n",
    "\n",
    "    def get_news_insights(self, symbol): # This method retrieves all market news insights for a given symbol\n",
    "        results=self.news_insights.get(symbol, [])\n",
    "        if not results:\n",
    "            print(f\"No news insights found for symbol {symbol}.\")\n",
    "            return []\n",
    "        if results:\n",
    "            filtered_results = []\n",
    "            for result in results:\n",
    "                # if the timestamp is older than 2 days, we can choose to ignore it\n",
    "                timestamp = datetime.fromisoformat(result['timestamp'])\n",
    "                if (datetime.now() - timestamp).days > 2:\n",
    "                    continue\n",
    "                filtered_results.append(result)\n",
    "        return filtered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf287589-30f8-4c56-a61c-175eeaf9af11",
   "metadata": {},
   "source": [
    "### 4. AGENTS\n",
    "#### For our routing workflow, along with communication also comes specialization and tool usage: a team of agents that will collaborate, coordinated by the main orchestrator agent. That's what we'll define in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13a923fc-91cd-40c0-aeb6-55eb20d0bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we'll initialize the Google GenAI and OpenAI\n",
    "from google import genai\n",
    "import openai\n",
    "#Make sure to load the environmental variables\n",
    "dotenv.load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "# Downloading necessary libraries and functionality - uncomment when needed.\n",
    "#nltk.download('vader_lexicon')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "class Agent: # This will be our base class for all our agents\n",
    "    def __init__(self, name, role, system_prompt, model, generate_response, agents=None, tools=None, memory_system=None, parser=None, debug=0): # This is the initialization method of the Agent class\n",
    "        self.name = name # Placeholder for the name of the tool\n",
    "        self.model = model # Placeholder for the LLM model\n",
    "        self.role = role # Placeholder for the role of this agent\n",
    "        self.system_prompt = system_prompt # Placeholder for the system prompt that defines this agent\n",
    "        self.memory_system = memory_system # Placeholder for the memory object for this agent - it could be None, so the agent would start without knowledge\n",
    "        self.parser = parser  # Placeholder for API details when needed\n",
    "        self.generate_response = generate_response # Placeholder for the generate response method\n",
    "        self.agents = agents \n",
    "        self.tools = tools # Placeholder for the tools passed on to this agent, which should be a list\n",
    "        self.conversation_history = [] # Initializing a blank conversation history\n",
    "        self.max_history_length = 10 # Initializing a default max number of history length\n",
    "\n",
    "        self.prompt_template = (\n",
    "            \"You are {agent_name}, an AI agent. Use the following tools as needed:\\n\"\n",
    "            \"{tools}\\n\"\n",
    "            \"Conversation history:\\n\"\n",
    "            \"{history}\\n\"\n",
    "            \"Current input: {input}\\n\"\n",
    "            \"Respond appropriately.\"\n",
    "        )\n",
    "        self.initialize_client() #Initializing the LLM client\n",
    "        self.debug = debug #Setting the debug local variable, used to print certain validation statements when set to 1\n",
    "    #We want our Agent class to support multiple LLMs, so this function will help initialize its internal client dynamically.\n",
    "    def initialize_client(self):\n",
    "        #For GPT models\n",
    "        if \"gpt\" in self.model.lower(): \n",
    "            self.client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        #For Gemini models\n",
    "        elif \"gemini\" in self.model.lower():\n",
    "            self.client = genai.Client()\n",
    "    def to_dict(self): # The structure of each class will always be a standard dictionary object that can be easily interpreted by the Agents\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"description\": self.description,\n",
    "            \"api\": self.api\n",
    "        }\n",
    "    def register_tool(self, tool): #This function helps register tools that the agent will have access to.\n",
    "        self.tools.append(tool) \n",
    "    def remember(self, message): #This function enables the agent to remember a message in its conversation history\n",
    "        self.conversation_history.append(message)\n",
    "        if len(self.conversation_history) > self.max_history_length:\n",
    "            self.conversation_history.pop(0)\n",
    "    def call_llm(self, input_prompt): #This is the generic call to LLM that agents can use. They may have a different version if needs are unique\n",
    "        try:\n",
    "            #For GPT models\n",
    "            if \"gpt\" in self.model.lower(): \n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": input_prompt}\n",
    "                    ],\n",
    "                    max_tokens=300,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                result = response.choices[0].message.content\n",
    "            #For Gemini models\n",
    "            elif \"gemini\" in self.model.lower():\n",
    "                prompt = self.system_prompt\n",
    "                prompt += \"\\n\" + input_prompt\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=self.model, contents=str(prompt)\n",
    "                )\n",
    "                result = response.text\n",
    "            #print(f\"{self.name} using model '{self.model}': {result[:60]}...\")\n",
    "            #print(result)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if self.debug == 1:\n",
    "                print(f\" API failed for {self.name} using model '{self.model}': {e}\")\n",
    "            return f\"Mock response from {self.name} with model '{self.model}': {input_prompt[:50]}...\"\n",
    "    def generate_response(self, **kwargs): # This is the placeholder of the generative function for the agent, which will receive a variable number of parameters\n",
    "        if self.debug == 1:\n",
    "            print(f\"Invoking {self.name} generative response function with arguments {kwargs}\")\n",
    "        return self.generate_response(**kwargs) # Returning the results of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fed17b1-9690-4054-8131-025b22fe39fb",
   "metadata": {},
   "source": [
    "#### Next, let's define some Sub-agents that will inherit from the class above.\n",
    "#### We'll start with the Market Research agent, capable of finding hard-financial data like stock quotes or market trends using some of the financial tools declared at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b97db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketResearchAgent(Agent):\n",
    "    def __init__(self, model=\"gemini-2.5-flash\", debug=0):\n",
    "        name=\"Market Research Agent\"\n",
    "        model=model\n",
    "        role=\"Market Research Agent specialized in financial data analysis and market trends\"\n",
    "        system_prompt=f\"\"\"You are a Market Research Agent specialized in financial data analysis and market trends.\n",
    "         Your role is to assist users by providing accurate and up-to-date financial information, stock quotes, market trends, and insights based on the latest data available from various financial APIs and tools.\n",
    "\n",
    "         Based on the data retrieved from the tools at your disposal, provide comprehensive answers to user queries related to stock performance, market analysis, and financial news.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.memory_system=MemorySystem()\n",
    "        super().__init__(name=name,system_prompt=system_prompt,model=model,generate_response=self.generate_response,role=role,agents=None,tools=None,memory_system=self.memory_system,parser=None, debug=debug) \n",
    "    \n",
    "    def generate_response(self, **kwargs): # This is the placeholder of the generative function for the agent, which will receive a variable number of parameters\n",
    "        if self.debug == 1:\n",
    "            print(f\"Invoking {self.name} generative response function with arguments {kwargs}\")\n",
    "        input_prompt=kwargs.get(\"prompt\",[])\n",
    "        try:\n",
    "            #For GPT models\n",
    "            if \"gpt\" in self.model.lower(): \n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": input_prompt}\n",
    "                    ],\n",
    "                    #messages=input_prompt,\n",
    "                    max_tokens=300,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                result = response.choices[0].message.content\n",
    "            #For Gemini models\n",
    "            elif \"gemini\" in self.model.lower():\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=self.model, contents=str(input_prompt)\n",
    "                )\n",
    "                result = response.text\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if self.debug == 1:\n",
    "                print(f\" API failed for {self.name} using model '{self.model}': {e}\")\n",
    "            return f\"Mock response from {self.name} with model '{self.model}': {input_prompt[:50]}...\"\n",
    "\n",
    "    def getMarketSummary(self,symbol:str  ) -> str:\n",
    "        prompt=f\"\"\"Provide a comprehensive market summary for the stock symbol: {symbol}. \n",
    "                Include recent performance, key financial metrics, and any notable news or trends affecting the stock.\n",
    "                Use data from Yahoo Finance, Financial Modeling Prep, and FinnHub to inform your summary.\n",
    "                Format the response in a clear and concise manner suitable for a financial report.\"\"\"\n",
    "        insights = self.memory_system.get_stock_insights(symbol)\n",
    "        if insights:\n",
    "            if self.debug == 1:\n",
    "                print(f\"Using cached insight for symbol {symbol}.\")\n",
    "            return insights[-1]['insight']\n",
    "        else:\n",
    "            tools_list=[FinancialScore(),IncomeStatement(),StockQuote(),StockPriceChange()]\n",
    "            for tool in tools_list:\n",
    "                tool_response=tool.invoke(symbol=symbol)\n",
    "                prompt+=f\"\\nData from {tool.name}: {tool_response}\"\n",
    "            response=self.generate_response(prompt=prompt)\n",
    "            self.memory_system.add_stock_insight(symbol, response,timestamp=datetime.now().isoformat())\n",
    "        return response  \n",
    "    \n",
    "    def  processUserInput(self, user_input: str) -> str:\n",
    "        tags=self.getEntities(user_input=user_input)\n",
    "        if \"symbol\" in tags:\n",
    "            marketSummary=self.getMarketSummary(symbol=tags.get(\"symbol\"))\n",
    "        prompt=f\"\"\"Based on the {marketSummary} Analyze the following user input\n",
    "                and provide a short answer for the user query.\n",
    "                Rules:\n",
    "                - If the user input is related to stock performance, provide insights based on the market summary.\n",
    "                - If the user input is unrelated to financial markets, respond with \"I'm sorry, I can only assist with financial market-related queries.\"\n",
    "                - Keep the response concise and relevant to the user's query.\n",
    "                - Use a professional and informative tone suitable for financial discussions.\n",
    "                - Limit the response to 150 words.\n",
    "\n",
    "                User Input: \"{user_input}\"\n",
    "\n",
    "\n",
    "                Answer:\n",
    "                \"\"\",\n",
    "        response=self.generate_response(prompt=prompt)\n",
    "        return response\n",
    "\n",
    "    def getEntities(self, user_input: str) -> str:\n",
    "        prompt=f\"\"\"Determine entities the following user input related to financial markets and stock analysis:\n",
    "                if the input contains Apple Inc, return SYMBOL as AAPL\n",
    "                if the input contains Microsoft Corporation, return SYMBOL as MSFT\n",
    "                User Input: \"{user_input}\n",
    "                Extracted Entities:\n",
    "                    <SYMBOL>...</SYMBOL>\n",
    "                    <EXCHANGE>...</EXCHANGE><INDUSTRY>...</INDUSTRY>  \"\"\"\n",
    "        response=self.generate_response(prompt=prompt)\n",
    "        if self.debug == 1:\n",
    "            print(f'Response: {response}')\n",
    "        parser=XmlParser()\n",
    "        parsed_response=parser.parseTags(response)\n",
    "        return parsed_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba108b-b298-4412-a752-a17970a6d742",
   "metadata": {},
   "source": [
    "#### Then, we'll define a Market Sentiment Agent, which will pull financial news using some of the financial tools above, and classify their sentiment, which will be helpful for the research and analsys of the company in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb24a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketSentimentAgent(Agent):\n",
    "    def __init__(self, model=\"gemini-2.5-flash\", debug=0):\n",
    "        name=\"Market News Sentiment Agent\"\n",
    "        model=model\n",
    "        role=\"Market News Sentiment Agent specialized in financial news sentiment analysis\"\n",
    "        system_prompt=f\"\"\"You are a Market Sentiment Agent specialized in financial news sentiment analysis.\n",
    "         Your role is to assist users by analyzing the sentiment of financial news articles and providing insights based on the emotional tone of the content.\n",
    "\n",
    "         Based on the news data retrieved from FinnHub, provide comprehensive sentiment analysis to help users understand market mood and potential impacts on stock performance.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.memory_system=MemorySystem()\n",
    "        super().__init__(name=name,system_prompt=system_prompt,model=model,generate_response=self.generate_response,role=role,agents=None,tools=None,memory_system=self.memory_system,parser=None, debug=debug)\n",
    "        \n",
    "    def generate_response(self, **kwargs): # This is the placeholder of the generative function for the agent, which will receive a variable number of parameters\n",
    "        if self.debug==1:\n",
    "            print(f\"Invoking {self.name} generative response function with arguments {kwargs}\")\n",
    "        input_prompt=kwargs.get(\"prompt\",[])\n",
    "        try:\n",
    "            #For GPT models\n",
    "            if \"gpt\" in self.model.lower(): \n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=input_prompt,\n",
    "                    max_tokens=300,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                result = response.choices[0].message.content\n",
    "            #For Gemini models\n",
    "            elif \"gemini\" in self.model.lower():\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=self.model, contents=str(input_prompt)\n",
    "                )\n",
    "                result = response.text\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if self.debug==1:\n",
    "                print(f\" API failed for {self.name} using model '{self.model}': {e}\")\n",
    "            return f\"Mock response from {self.name} with model '{self.model}': {input_prompt[:50]}...\"\n",
    "        \n",
    "    def getNewsSummary(self,symbol:str  ) -> str:\n",
    "            prompt=f\"\"\"Provide a comprehensive news summary for the stock symbol: {symbol}.\n",
    "                    Include recent news articles, key events, and any notable trends affecting the stock.\n",
    "                    Use data from FinnHub and other news sources to inform your summary.\n",
    "                    Format the response in a clear and concise manner suitable for a financial report.\"\"\"\n",
    "            insights = self.memory_system.get_news_insights(symbol)\n",
    "            if insights:\n",
    "                if self.debug==1:\n",
    "                    print(f\"Using cached insight for symbol {symbol}.\")\n",
    "                return insights[-1]['news_item']\n",
    "            else:\n",
    "                tools_list=[FinancialNews(),RecommendationTrends(),EarningSurprise()]\n",
    "                for tool in tools_list:\n",
    "                    tool_response=tool.invoke(symbol=symbol)\n",
    "                    prompt+=f\"\\nData from {tool.name}: {tool_response}\"\n",
    "                response=self.generate_response(prompt=prompt)\n",
    "                self.memory_system.add_market_news(symbol, response,timestamp=datetime.now().isoformat())\n",
    "            return response\n",
    "        \n",
    "    def processUserInput(self, user_input: str) -> str:\n",
    "        if self.debug==1:\n",
    "            print(\"-\" * 50)\n",
    "            print(f'{self.name}\" received input: {user_input}')\n",
    "            print(\"-\" * 50)\n",
    "        tags=self.getEntities(user_input=user_input)\n",
    "        if \"symbol\" in tags:\n",
    "            newsSummary=self.getNewsSummary(symbol=tags.get(\"symbol\"))\n",
    "        prompt=f\"\"\"Based on the {newsSummary} Analyze the following user input\n",
    "                and provide a short answer for the user query.\n",
    "                Rules:\n",
    "                - If the user input is related to financial news sentiment, provide insights based on the news summary.\n",
    "                - If the user input is unrelated to financial markets, respond with \"I'm sorry, I can only assist with financial market-related queries.\"\n",
    "                - Keep the response concise and relevant to the user's query.\n",
    "                - Use a professional and informative tone suitable for financial discussions.\n",
    "                - Limit the response to 150 words.\n",
    "\n",
    "                User Input: \"{user_input}\"\n",
    "\n",
    "\n",
    "                Answer:\n",
    "                \"\"\",\n",
    "        response=self.generate_response(prompt=prompt)\n",
    "        return response\n",
    "    def getEntities(self, user_input: str) -> str:\n",
    "        prompt=f\"\"\"Determine entities the following user input related to financial markets and stock analysis:\n",
    "                if the input contains Apple Inc, return SYMBOL as AAPL\n",
    "                if the input contains Microsoft Corporation, return SYMBOL as MSFT\n",
    "                User Input: \"{user_input}\n",
    "                Extracted Entities:\n",
    "                    <SYMBOL>...</SYMBOL>\n",
    "                    <EXCHANGE>...</EXCHANGE><INDUSTRY>...</INDUSTRY>  \"\"\"\n",
    "        response=self.generate_response(prompt=prompt)\n",
    "        parser=XmlParser()\n",
    "        parsed_response=parser.parseTags(response)\n",
    "        return parsed_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d0b9f-ecd8-4d6c-a103-acd402e022ef",
   "metadata": {},
   "source": [
    "#### Then, we'll define a Writer agent in charge of providing a polished and structured answer with the researched data provided by the other agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ac90380-12c0-4734-8642-c0c0445c230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WriterAgent(Agent):\n",
    "    # This agent takes the results of other agents (like news or market research) and creates a professional report that will be returned to the Orchestrator for the Final Response to the user.\n",
    "    def __init__ (self, model=\"gpt-3.5-turbo\", agents=None, memory_system=None, debug=0):\n",
    "        super().__init__(\n",
    "            name=\"Writer\", #Name of the Writer class\n",
    "            role=\"Writer Agent specialized in polished Financial Content\", #Role of this class\n",
    "            system_prompt=(\n",
    "                \"You are Writer, an AI agent part of an agents team. Your role is a professional \"\n",
    "                \"financial report writer, capable of taking financial news \"\n",
    "                \"or financial information provided by the Orchestrator and \"\n",
    "                \"preparing a 2–3 paragraph report that provides a clear final \"\n",
    "                \"answer to the user.\"\n",
    "                \"Here are some guidelines for you:\"\n",
    "                \"Start your answers giving a positive message like 'Great question', 'Excellent question', or similar.\"\n",
    "                \"Focus on answering the user's question.\"\n",
    "                \"When recommendations are requested, only provide guidance and highlight pros and cons.\"\n",
    "                \"The news or financial information you're receiving came from other agents in the team, so never refer to it as 'the data provided'.\"\n",
    "            ),            \n",
    "            model = model,\n",
    "            generate_response = self.generate_response,\n",
    "            memory_system = memory_system,\n",
    "            debug=debug\n",
    "        )\n",
    "    def generate_response(self, input_prompt):\n",
    "        result = self.call_llm(input_prompt)\n",
    "        return result\n",
    "    def processUserInput(self, input_prompt: str) -> str:\n",
    "        prompt=input_prompt\n",
    "        response=self.generate_response(input_prompt=prompt)\n",
    "        return response    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d208600-b43d-424f-b54d-7ee6f97c1257",
   "metadata": {},
   "source": [
    "### 5. MAIN ORCHESTRATOR AGENT.\n",
    "#### This is the section where we define the orchestrator agent, which performs the interpretation of the user's prompt, prepares a plan, calls the subagents as needed, and prepares the final answer to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8fa6bcc-d542-4354-80bc-95ad0e9cf6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrchestratorAgent(Agent):\n",
    "    def __init__(self, model, agents=None, memory=None, parser=None, debug=0):\n",
    "        self.agents = agents\n",
    "        #self.agents_description = \"\\n\".join([f\"- {agent.name}: {agent.role}\" for agent in self.agents.items()])\n",
    "        self.agents_description = \"\"\n",
    "        for agent in self.agents:\n",
    "            self.agents_description += f'\\n- {agent.name}: {agent.role}'\n",
    "\n",
    "        system_prompt = f'''\n",
    "          You are the leading AI agent for the following team of agents:\n",
    "            {self.agents_description}\n",
    "\n",
    "            You do not generate a response directly to the user, but instead you'll coordinate the agents team by generating a list of tasks for them to do following the Agent Usage guidelines.\n",
    "            \n",
    "            ### Agent Usage Guidelines:\n",
    "\n",
    "            1. Do not respond to the current input directly. Instead, create a plan to call the research agents in your team to pull the necessary data.\n",
    "            2. Convert that plan into a list of calls for your specialized agents (except for the Writer Agent) using an XML structure with the tag \"<SpecializedAgent>\" in the following format:\n",
    "            <SpecializedAgent>{{\"agentName\": \"Market Research Agent\", \"user_input\": \"Your specific query here\"}}</SpecializedAgent>\n",
    "            3. The writer agent will be called separately to finalize the response. Exclude from your thinking process.\n",
    "            \n",
    "            ### Other TAGs you can include in your plan:\n",
    "            -  For thinking, you must wrap your thoughts in <Thought> and </Thought> tags.\n",
    "            -  For final answers, you must wrap your answer in <FinalAnswer> and </FinalAnswer> tags.\n",
    "            -  If you need users to provide more information, you must wrap your request in <RequestMoreInfo> and </RequestMoreInfo> tags.\n",
    "           \n",
    "            ### Instructions for using the tools:\n",
    "            You should only use the information returned by the Agents listed above, never try to get information independently.\n",
    "        '''\n",
    "        system_prompt = system_prompt.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        # We also need to declare a parser\n",
    "        if parser==None:\n",
    "            parser = XmlParser()\n",
    "\n",
    "        super().__init__(\n",
    "            name = \"Orchestrator Agent\", #Name of the Orchestrator class\n",
    "            role = \"Orchestrator Agent that manages tool usage and conversation flow\",\n",
    "            system_prompt = system_prompt,\n",
    "            model = model,\n",
    "            generate_response = self.generate_response,\n",
    "            memory_system = memory,\n",
    "            agents = agents,\n",
    "            parser = parser,\n",
    "            debug = debug # Storing the variable debug, used for printing messages when set to 1\n",
    "        )\n",
    "        self.conversation_history = []\n",
    "            # Limit for conversation history\n",
    "        # print(f\"Prompt Template: {self.system_prompt}\")\n",
    "        self.prompt_template = (\n",
    "            f\"{self.system_prompt}\\n\"\n",
    "            \"Conversation history:\\n\"\n",
    "            \"{history}\\n\"\n",
    "            \"Current input: {input}\\n\"\n",
    "        )\n",
    "        self.parser = parser\n",
    "        self.initialize_client()\n",
    "\n",
    "    def remember(self, message):\n",
    "        self.conversation_history.append(message)\n",
    "        if len(self.conversation_history) > self.max_history_length:\n",
    "            self.conversation_history.pop(0)\n",
    "    \n",
    "    def generate_response(self, input_prompt):\n",
    "        history_text = \"\\n\".join(self.conversation_history)\n",
    "        #print(\"Conversation History: \")\n",
    "        #print(history_text)\n",
    "        response = \"\"\n",
    "        prompt = self.prompt_template.format(\n",
    "            history=history_text,\n",
    "            input=input_prompt\n",
    "        )\n",
    "        if self.debug==1:\n",
    "            print(f\"Orchestrator Prompt: {prompt}\")\n",
    "        try:\n",
    "            #For GPT models\n",
    "            if \"gpt\" in self.model.lower(): \n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    #messages=input_prompt,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": prompt}\n",
    "                    ],\n",
    "                    max_tokens=300,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                result = response.choices[0].message.content\n",
    "            #For Gemini models\n",
    "            elif \"gemini\" in self.model.lower():\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=self.model, contents=str(input_prompt)\n",
    "                )\n",
    "                result = response.text\n",
    "            self.remember(f\"User: {input_prompt}\")\n",
    "            self.remember(f\"{self.name}: {response}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if self.debug==1:\n",
    "                print(f\" API failed for {self.name} using model '{self.model}': {e}\")\n",
    "            return f\"Mock response from {self.name} with model '{self.model}': {input_prompt[:50]}...\"\n",
    "        return response\n",
    "    \n",
    "\n",
    "    def get_specialist_opinion(self, agentName, user_input):\n",
    "        '''Agent Orchestrator can call other agents to get their opinion on specific user inputs.'''\n",
    "        MyAgentsTeam = {MyMarketResearcher, MyNewsResearcher, MyWriter}\n",
    "        for agent in self.agents:\n",
    "            if agent.name == agentName:\n",
    "                return agent.processUserInput(user_input)\n",
    "        return f\"Agent {agentName} not found.\"\n",
    "        \n",
    "    \n",
    "    def reAct(self, user_input:str)-> str:\n",
    "        # Here is the the logic to parse the response for Agents usage\n",
    "        # and store the results.\n",
    "        parsed_response = \"\"\n",
    "        #Preparing a temporary repository for agent responses\n",
    "        temp_agent_response = \"\"\n",
    "        temp_agent_response_count = 0\n",
    "        #We also initialize the content variable we'll pass to the writer\n",
    "        content_for_writer = f'Current user prompt: {user_input}'\n",
    "        if self.parser and self.agents:\n",
    "            response = self.generate_response(user_input)\n",
    "            parsed_response = self.parser.parse_all(response) ## parsed response is a dict {\"InvokeTool\": \"tool_name\", \"parameters\": {...}} or {\"FinalAnswer\": \"answer\"} or {\"RequestMoreInfo\": \"info\"}\n",
    "            if self.debug==1:\n",
    "                print(\"*\" * 50)\n",
    "                print(f'Raw actions from Orchestrator: {response}')\n",
    "                print(\"*\" * 50)\n",
    "                print(\"*\" * 50)\n",
    "                print(f'Actions list from Orchestrator: {parsed_response}')\n",
    "                print(\"*\" * 50)\n",
    "            system_message = f\"System: {response}\"\n",
    "            self.remember(system_message)\n",
    "            self.conversation_history.append(system_message)\n",
    "            '''\n",
    "            parsed_response= {\n",
    "            \"action\": \"InvokeTool\",\n",
    "            \"parameters\": {\n",
    "                \"symbol\": \"AAPL\",\n",
    "                \"step\": \"financials\"\n",
    "                }\n",
    "            }\n",
    "            '''\n",
    "            # Next, we'll loop through all the actions in the plan to execute one at a time.\n",
    "            for plan_item in parsed_response:\n",
    "                action = plan_item.get(\"action\")\n",
    "                if self.debug==1:\n",
    "                    print(f\"Orchestrator Action: {action}\")\n",
    "                if action == \"SpecializedAgent\":\n",
    "                    agent_name = plan_item[\"parameters\"].get(\"agentName\")\n",
    "                    user_input_for_agent = plan_item[\"parameters\"].get(\"user_input\")\n",
    "                    if self.debug==1:\n",
    "                        print(\"-\" * 50)\n",
    "                        print(f'Orchestrator calling {agent_name} with prompt \"{user_input_for_agent}\"')\n",
    "                        print(\"-\" * 50)\n",
    "                    agent_response = self.get_specialist_opinion(agent_name, user_input_for_agent)\n",
    "                    temp_agent_response = f\"Agent {agent_name} Response: {agent_response}\"\n",
    "                    self.remember(temp_agent_response)\n",
    "                    self.conversation_history.append(temp_agent_response)\n",
    "                    content_for_writer += f'\\n\\n{temp_agent_response}'\n",
    "                    # Generate a new response based on the agent result\n",
    "                    #response = self.generate_response(f\"Agent {agent_name} Response: {agent_response}\")\n",
    "                    #parsed_response = self.parser.parse(response)   \n",
    "                    temp_agent_response_count += 1\n",
    "                elif action == \"FinalAnswer\" or action == \"RequestMoreInfo\" or action == \"NeedApproval\":\n",
    "                    print(f\"Orchestrator Final Response: {response}\")\n",
    "                    return parsed_response.get(\"content\")\n",
    "                elif action == \"Thought\":\n",
    "                    continue\n",
    "                else:\n",
    "                    return f\"I'm not sure how to proceed. Could you please clarify? - selected action: {action}\"\n",
    "            #Once the loop of actions is completed, we'll pass the information gathered by all research agents down to our writer\n",
    "            #user_input_for_agent = \n",
    "            response = self.get_specialist_opinion('Writer', content_for_writer)\n",
    "        else:\n",
    "            parsed_response = \"Error: no parser or sub agents found!\"\n",
    "            print('Parser:')\n",
    "            print(self.parser)\n",
    "            print('Agents:')\n",
    "            print(self.agents)\n",
    "            response = parsed_response\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b7fae3d-16cf-4e77-9fd0-40f33e0ad6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Excellent question! Deciding whether to invest in a dynamic stock like Tesla (TSLA) today requires a careful look at both its impressive historical trajectory and the immediate market sentiment. While the stock has shown remarkable long-term growth and strong recent performance, the current environment also presents notable points of caution for potential investors, particularly with a key financial event approaching.\\n\\nFrom a performance perspective, Tesla stock has delivered exceptional returns over various periods. It\\'s currently trading higher today and has demonstrated strong positive trends across the short, medium, and long term, including a near-doubling in value over the past year and more than tripling over five years. This robust historical performance indicates significant investor confidence and a capacity for substantial growth, reflecting its innovative position in the EV market and beyond.\\n\\nHowever, the immediate outlook is marked by a cautious sentiment among market observers. Tesla is at a \"make-or-break point\" ahead of its Q3 earnings report next week, with specific concerns around a potential \"front-loading effect\" and a recent rating downgrade. Broader market factors like persistent high EV costs, US-China tensions, and general volatility are also contributing to this caution. Furthermore, while analyst recommendations are net positive, a significant portion are \"Hold\" ratings, reflecting this overall uncertainty, compounded by the company\\'s history of earnings misses. Therefore, while Tesla\\'s long-term potential remains attractive, buying today involves weighing the proven growth against these immediate risks and the upcoming earnings report, which could significantly impact its near-term price. Investors are encouraged to consider their own risk tolerance and investment goals.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can use \"gpt-3.5-turbo\" or \"gemini-2.5-flash\", #Uncomment if using Google GenAI\n",
    "\n",
    "#Let's declare our sub agent instances first.\n",
    "MyMarketResearcher = MarketResearchAgent(model=\"gemini-2.5-flash\")\n",
    "MyNewsResearcher = MarketSentimentAgent(model=\"gemini-2.5-flash\")\n",
    "MyWriter = WriterAgent(model=\"gemini-2.5-flash\")\n",
    "\n",
    "#We put all of our sub agents together as a list of objects\n",
    "MyAgentsTeam = {MyMarketResearcher, MyNewsResearcher, MyWriter}\n",
    "\n",
    "#Now, we declare our main orchestrator agent instance.\n",
    "MyOrchestrator = OrchestratorAgent(model=\"gpt-3.5-turbo\", agents=MyAgentsTeam)\n",
    "sample_prompt = \"Based on the latest news and stock prices, it is a good time to buy Tesla stock today?\"\n",
    "\n",
    "MyOrchestrator.reAct(sample_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abea2e1e-d811-44cb-83eb-1afd0d66f720",
   "metadata": {},
   "source": [
    "### 6. DEMO.\n",
    "#### This is the final section, which contains the implementation of the entire system using all elements above for a quick demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed6e69f-4a54-4dc9-ade0-1c38031f6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_tools = [tools.YahooFinance(), tools.FMP(), tools.FinnHub()]\n",
    "toolsList = [ tool.to_dict() for tool in registered_tools ]\n",
    "executionMap= { tool.name: tool for tool in registered_tools }\n",
    "\n",
    "executionMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbc0c6-0f5d-477f-9e3e-925cef221d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.parser as parser\n",
    "parser = parser.XmlParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a4fff4-e92e-4b1f-a976-497ffa407111",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = systemPrompt\n",
    "userText =\"\"\n",
    "continueFlag = False\n",
    "exitFlag = \"\"\n",
    "debug = False\n",
    "while userText.lower() != \"exit\" or exitFlag.lower() != \"y\":\n",
    "    if not continueFlag:\n",
    "        userText = input(\"User: \")\n",
    "        if userText.lower() == \"exit\":\n",
    "            break\n",
    "        prompt += \"\\n User:\"+ userText\n",
    "        continueFlag = False\n",
    "    if debug:\n",
    "        print('Prompt: ')\n",
    "        print(prompt)\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\", contents=prompt\n",
    "    )\n",
    "    if debug:\n",
    "        print('Model response: ')\n",
    "        print(response.text)\n",
    "    prompt = prompt + response.text\n",
    "    actions = parser.parse_all(response.text.replace('\\n', ' '))\n",
    "    if debug:\n",
    "        print('Actions: ')\n",
    "        print(actions)\n",
    "    if not actions:\n",
    "        continue\n",
    "    if len(actions) >= 1:\n",
    "        actions = { action['action']: action for action in actions if action.get(\"action\",\"\") != \"Final Answer\" }\n",
    "        if \"NeedApproval\" in actions:\n",
    "            actions.pop(\"NeedApproval\")\n",
    "            if \"InvokeTool\" in actions:\n",
    "                #result=input(\"Need to Call \" + actions[\"InvokeTool\"].get(\"name\",\"\") + \" Y/y to continue...)\")\n",
    "                result=input(\"Need to Call \" + actions[\"InvokeTool\"]['parameters']['name'] + \". Type Y/y to continue...)\")\n",
    "            else:\n",
    "                result=input(\"Need User Approval Y/y to continue...)\")\n",
    "            if result.lower() != \"y\":\n",
    "                print(\"Exiting...\")\n",
    "                break\n",
    "            actions[\"NeedApproval\"] = {\"action\":\"NeedApproval\", \"content\":\"User approved to continue.\"}\n",
    "            prompt += \"\\n User:\"+ actions[\"NeedApproval\"].get(\"content\",\"\")\n",
    "        if 'InvokeTool' in actions:\n",
    "            action = actions[\"InvokeTool\"]\n",
    "            tools_name = action[\"parameters\"][\"name\"]\n",
    "            tool_params = action[\"parameters\"][\"api\"]\n",
    "            result = executionMap[tools_name].invoke(**json.loads(tool_params))\n",
    "            actions[\"Tool result\"] = {\"action\":\"Tool result\", \"content\":result}\n",
    "            prompt += \"\\n User:\"+ str(result)\n",
    "            continueFlag = True\n",
    "        if \"FinalAnswer\" in actions:\n",
    "            content = actions[\"FinalAnswer\"]['parameters']['content']\n",
    "            print(\"Final Answer: \"+ str(content))\n",
    "            exitFlag = input(\"Do you want to exit? Type Y/y to exit...\")\n",
    "            if exitFlag.lower() == \"y\":\n",
    "                print('Thanks for chatting! Goodbye!')\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a113ed-09dc-4d20-ba76-baa879315c15",
   "metadata": {},
   "source": [
    "## CONCLUSION.\n",
    "#### Enter our conclusions here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
